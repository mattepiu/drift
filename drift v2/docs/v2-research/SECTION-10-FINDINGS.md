# Section 10 Findings: Phase Estimates, Build Ordering & Verification Gates (Phases 0-4)

> **Status:** âœ… DONE
> **Date completed:** 2026-02-08
> **Orchestration plan sections:** Â§3-7 (Phases 0-4), Â§19 (Verification Gates M1-M4)
> **Round:** 2 (Orchestration-level validation)
>
> **Summary: 10 CONFIRMED, 5 REVISE, 0 REJECT, 6 APPLIED**

---

## Checklist (all validated)

- [x] Phase 0 estimate "1-2 weeks" â€” realistic given 6 crates (not 5) and all version bumps?
- [x] Phase 0 verification gate â€” are all 8 criteria testable and sufficient?
- [x] Phase 1 estimate "2-3 weeks" â€” realistic given scanner+parsers+storage+NAPI pipeline?
- [x] Phase 1 strict sequential ordering (Scannerâ†’Parsersâ†’Storageâ†’NAPI) â€” truly required or can overlap?
- [x] Phase 1 verification gate â€” are all 9 criteria testable? Is "10K files <3s" the right target?
- [x] Phase 2 estimate "3-4 weeks for core" â€” does this account for GAST expansion to ~40-50 types?
- [x] Phase 2 two-track parallelization â€” confirmed safe, verify convergence point
- [x] Phase 2 verification gate â€” 10 criteria, are they all measurable?
- [x] Phase 3 estimate "3-4 weeks" â€” realistic given the internal dependency chain?
- [x] Phase 3 internal ordering (Aggregationâ†’Confidenceâ†’Outliers/Learning) â€” any flexibility?
- [x] Phase 3 verification gate â€” 11 criteria, are they all measurable?
- [x] Phase 4 estimate "4-6 weeks" â€” realistic given 5 parallel systems?
- [x] Phase 4 "all 5 are parallel" claim â€” verify no hidden dependencies
- [x] Phase 4 verification gate â€” 12 criteria, are they all measurable?
- [x] Milestones M1-M4 timing â€” still accurate after Round 1 revisions?
- [x] Apply Round 1 revisions for Phases 0-4

---

## Findings

### 1. Phase 0 Estimate "1-2 weeks" â€” âœ… CONFIRMED

The plan estimates 1-2 weeks for one developer to scaffold 5 crates (now 6 with
drift-context from Round 1 OD-1) and implement 5 infrastructure primitives
(Configuration, thiserror, tracing, DriftEventHandler, String Interning).

Breakdown:
- Cargo workspace scaffold + dependency pinning: ~0.5 days
- drift-context crate (Round 1 addition): ~0.5 days (minimal scaffolding)
- DriftConfig with 4-layer resolution: ~1-1.5 days
- thiserror error enums (12+ enums with DriftErrorCode): ~1 day
- tracing + EnvFilter setup: ~0.5 days
- DriftEventHandler trait (21 methods with no-op defaults): ~1 day
- String interning (lasso ThreadedRodeo + PathInterner): ~0.5-1 day
- Version bumps (tree-sitter 0.25, rusqlite 0.38, petgraph 0.8, smallvec "1"): ~0.5 days (Cargo.toml edits)
- clippy + CI setup: ~0.5 days

Total: ~6-7 working days = ~1.2-1.4 weeks. The "1-2 weeks" estimate is realistic.
Adding the 6th crate (drift-context) adds ~0.5 days â€” well within the upper bound.

---

### 2. Phase 0 Verification Gate â€” âš ï¸ REVISE: Add 2 criteria

The plan lists 8 criteria (Â§3.7). All 8 are testable and automatable:

| # | Criterion | Testable? |
|---|-----------|-----------|
| 1 | `cargo build --workspace` succeeds with zero warnings | âœ… |
| 2 | `DriftConfig::load()` resolves 4 layers correctly | âœ… |
| 3 | Every error enum has a `DriftErrorCode` implementation | âœ… |
| 4 | `DRIFT_LOG=debug` produces structured span output | âœ… |
| 5 | `DriftEventHandler` trait compiles with no-op defaults | âœ… |
| 6 | `ThreadedRodeo` interns and resolves paths correctly | âœ… |
| 7 | All workspace dependencies are pinned at exact versions | âœ… |
| 8 | `cargo clippy --workspace` passes with zero warnings | âœ… |

Missing criteria after Round 1 revisions:
- **Criterion 9:** `panic = "abort"` is set in the release profile (Round 1 revision).
  Without this, unwinding through NAPI FFI boundary is UB.
- **Criterion 10:** drift-context crate compiles and exports its public types
  (Round 1 OD-1 resolution). This is the 6th crate â€” needs explicit verification.

**Recommendation:** Add criteria 9 and 10. Total: 10 criteria.

---

### 3. Phase 1 Estimate "2-3 weeks" â€” âœ… CONFIRMED

The plan estimates 2-3 weeks for one developer (1-2 weeks with two). The pipeline
is Scanner â†’ Parsers â†’ Storage â†’ NAPI.

Breakdown per Section 2 findings:
- Scanner (System 00): ~3-4 days (ignore crate integration, xxh3 hashing, incremental detection, .driftignore)
- Tree-Sitter Parsers (System 01): ~4-5 days (10 language grammars, ParseResult, query compilation, cache)
- SQLite Storage (System 02): ~3-4 days (WAL setup, batch writer, migration system, read pool)
- NAPI Bridge (System 03): ~2-3 days (singleton runtime, 3 core functions, AsyncTask)

Total sequential: 12-16 days = 2.4-3.2 weeks. With overlap (see finding #4),
achievable in 2-3 weeks. With 2 developers, 1.5-2 weeks is realistic.

---

### 4. Phase 1 Strict Sequential Ordering â€” âš ï¸ REVISE: Partial overlap possible

The plan states "No parallelism possible here â€” it's a strict pipeline" (Â§4).
This is overstated. While the integration testing is sequential (Scanner output
feeds Parsers, Parsers feed Storage, Storage feeds NAPI), development can overlap:

- Scanner and Storage can be developed in parallel â€” Storage's connection
  architecture, batch writer, and migration system are independent of what gets
  stored. The plan itself notes this: "The connection architecture, batch writer,
  and migration system are independent of what gets stored" (Â§4.3).
- Parsers and NAPI can overlap partially â€” the NAPI singleton pattern and
  AsyncTask infrastructure don't depend on parser output.

Section 8 findings confirmed this: "Storage + NAPI can overlap with Scanner +
Parsers during development."

**Recommendation:** Revise "No parallelism possible" to "Integration testing is
sequential (Scannerâ†’Parsersâ†’Storageâ†’NAPI), but development of Storage and NAPI
infrastructure can overlap with Scanner and Parsers."

---

### 5. Phase 1 Verification Gate â€” âœ… CONFIRMED (with performance target note)

The plan lists 9 criteria (Â§4.5). All are testable:

| # | Criterion | Testable? |
|---|-----------|-----------|
| 1 | `drift_initialize()` creates drift.db with correct PRAGMAs | âœ… |
| 2 | `drift_scan()` discovers files, computes hashes, returns ScanDiff | âœ… |
| 3 | Incremental scan correctly identifies added/modified/removed files | âœ… |
| 4 | All 10 language parsers produce valid ParseResult from test files | âœ… |
| 5 | Parse cache hits on second parse of unchanged file | âœ… |
| 6 | Batch writer persists file_metadata and parse results to drift.db | âœ… |
| 7 | `drift_shutdown()` cleanly closes all connections | âœ… |
| 8 | TypeScript can call all three functions and receive typed results | âœ… |
| 9 | Performance: 10K files scanned + parsed in <3s end-to-end | âœ… |

Criterion 9 ("10K files <3s") is the correct end-to-end target. Per Section 2
findings, the scanner-only target should distinguish platforms: macOS APFS has
higher filesystem latency than Linux ext4. The scanner-only target should be
~500ms on macOS (not 300ms as originally stated). The end-to-end <3s target
for scan+parse is achievable on all platforms.

---

### 6. Phase 2 Estimate "3-4 weeks for core" â€” âœ… CONFIRMED

The plan estimates 3-4 weeks for the core pipeline, with a warning that the full
UAE spans ~22 weeks across 7 internal phases (Â§5). The 3-4 week estimate covers:
- Core analysis engine + visitor pattern: ~1 week
- Initial detectors (5 categories, 50-80 detectors): ~1 week
- Call graph builder (6 resolution strategies): ~1 week
- Boundary detection + ULP: ~0.5-1 week

After Round 1 revision, GAST expansion from ~30 to ~40-50 node types adds work
to the normalizer layer. However, this is spread across the full UAE timeline
(22-27 weeks with Round 1 buffer), not concentrated in the Phase 2 core delivery.
The Phase 2 core only needs the base normalizer + 2-3 language normalizers.

The two-track parallelization (Track A: UAE+Detectors, Track B: CallGraph+Boundaries+ULP)
means the 3-4 week estimate is achievable with 2 developers working in parallel.

---

### 7. Phase 2 Two-Track Parallelization â€” âœ… CONFIRMED

Section 3 findings confirmed the two-track split is safe:

- **Track A** (Analysis + Detection): UAE â†’ Detector System. Tightly coupled â€”
  engine runs detectors as visitors.
- **Track B** (Graph + Boundaries): Call Graph + Boundary Detection + ULP.
  Depends on ParseResult but not on the detector system.

Convergence point: Phase 3 (Pattern Intelligence), which needs both detected
patterns (Track A output) and the call graph (Track B output). The convergence
is clean â€” Phase 3 reads from drift.db tables written by both tracks. No shared
mutable state between tracks during Phase 2.

---

### 8. Phase 2 Verification Gate â€” âœ… CONFIRMED

The plan lists 10 criteria (Â§5.8). All are testable and measurable:

| # | Criterion | Testable? | Measurable? |
|---|-----------|-----------|-------------|
| 1 | Analysis engine processes real codebase through all 4 phases | âœ… | âœ… |
| 2 | At least 5 detector categories produce valid PatternMatch | âœ… | âœ… |
| 3 | GAST normalization produces identical types for equivalent TS/Python | âœ… | âœ… |
| 4 | Call graph builds with all 6 resolution strategies | âœ… | âœ… |
| 5 | Incremental call graph update handles file changes | âœ… | âœ… |
| 6 | Boundary detection identifies ORM patterns across 5+ frameworks | âœ… | âœ… |
| 7 | ULP normalizes call chains across 3+ languages | âœ… | âœ… |
| 8 | All results persist to drift.db via batch writer | âœ… | âœ… |
| 9 | NAPI exposes drift_analyze() and drift_call_graph() | âœ… | âœ… |
| 10 | Performance: 10K file codebase analyzed in <10s | âœ… | âœ… |

The "10K files <10s" target (criterion 10) is tight but achievable with 50-80
initial detectors. As detector count grows toward 350+, this target may need
revisiting â€” but that's a Phase 3-5 concern, not Phase 2.

---

### 9. Phase 3 Estimate "3-4 weeks" â€” âœ… CONFIRMED

The plan estimates 3-4 weeks with limited parallelism due to the internal
dependency chain: Aggregation â†’ Confidence â†’ Outliers/Learning.

Breakdown:
- Pattern Aggregation (System 12): ~1 week (7-phase pipeline, Jaccard similarity, MinHash)
- Bayesian Confidence (System 10): ~1 week (Beta distribution, 5-factor model, momentum)
- Outlier Detection (System 11): ~0.5-1 week (6 methods, auto-selection, statrs integration)
- Learning System (System 13): ~0.5-1 week (convention discovery, Dirichlet extension)

Critical path: Aggregation (1w) â†’ Confidence (1w) â†’ max(Outliers, Learning) (1w) = 3 weeks.
The 4-week upper bound provides buffer for integration testing and edge cases.

The dependency chain is real â€” Confidence needs aggregated patterns, Outliers needs
confidence scores. But Outliers and Learning can run in parallel after Confidence
completes (see finding #10).

---

### 10. Phase 3 Internal Ordering â€” âœ… CONFIRMED

The dependency chain (Â§6.1) is:

```
Aggregation â†’ Confidence â†’ { Outliers, Learning } (parallel)
```

- Aggregation MUST come first: it turns per-file matches into project-level patterns.
  Without aggregated patterns, there's nothing to score.
- Confidence MUST follow Aggregation: it computes Beta posteriors on aggregated patterns.
- Outliers and Learning CAN be parallel after Confidence: Outliers uses confidence
  for threshold setting, Learning uses confidence for convention classification.
  Neither depends on the other.

No flexibility on the first two steps. The parallelism opportunity is only in the
last step (Outliers âˆ¥ Learning). This is correctly documented in the plan.

---

### 11. Phase 3 Verification Gate â€” âš ï¸ REVISE: Criterion 6 is vague

The plan lists 11 criteria (Â§6.6). 10 of 11 are testable and measurable:

| # | Criterion | Testable? | Measurable? |
|---|-----------|-----------|-------------|
| 1 | Pattern aggregation groups per-file matches into project-level patterns | âœ… | âœ… |
| 2 | Jaccard similarity correctly flags near-duplicate patterns (0.85) | âœ… | âœ… |
| 3 | Bayesian confidence produces Beta posteriors with correct tier classification | âœ… | âœ… |
| 4 | Momentum tracking detects rising/falling/stable trends | âœ… | âœ… |
| 5 | Outlier detection auto-selects correct method based on sample size | âœ… | âœ… |
| 6 | Z-Score, Grubbs', and IQR methods produce **statistically valid** results | âš ï¸ | âš ï¸ |
| 7 | Learning system discovers conventions with minOccurrences=3, dominance=0.60 | âœ… | âœ… |
| 8 | Convention categories classify correctly | âœ… | âœ… |
| 9 | All results persist to drift.db | âœ… | âœ… |
| 10 | NAPI exposes pattern query functions with keyset pagination | âœ… | âœ… |
| 11 | Performance: confidence scoring for 10K patterns in <500ms | âœ… | âœ… |

Criterion 6 says "statistically valid" without defining what that means. This is
not measurable as stated. A developer could implement Z-Score incorrectly and
still claim the results are "valid."

**Recommendation:** Replace criterion 6 with: "Z-Score, Grubbs', and IQR methods
produce correct results on a reference dataset of 50+ known outlier/non-outlier
samples (validated against R/scipy reference implementations)." This makes it
concrete and testable.

Note: criterion 11 ("10K patterns <500ms") is very conservative. Beta computation
is O(1) per pattern â€” 10K patterns should complete in <50ms. The 500ms budget
leaves room for database I/O and aggregation overhead.

---

### Phase 4 Dependency Analysis: "All 5 Are Parallel" â€” âœ… CONFIRMED

I constructed the full pairwise dependency matrix for the 5 Phase 4 systems:

| System A | System B | Dependency? | Analysis |
|----------|----------|-------------|----------|
| Reachability | Taint | Soft | Taint benefits from reachability for sensitivity classification of taint paths. But taint can be built with a stub reachability interface. |
| Reachability | Impact | None | Impact uses its own BFS traversal on the call graph. |
| Reachability | Error Handling | None | Error handling traces propagation chains independently. |
| Reachability | Test Topology | None | Test topology maps coverage via its own call graph BFS. |
| Taint | Impact | None | Impact doesn't consume taint results. |
| Taint | Error Handling | None | Independent analysis domains. |
| Taint | Test Topology | None | Independent analysis domains. |
| Impact | Error Handling | None | Independent analysis domains. |
| Impact | Test Topology | Soft | Impact benefits from test coverage data for "coverage gap" analysis. Can use stubs. |
| Error Handling | Test Topology | None | Independent analysis domains. |

**Hard dependencies:** Zero. All 5 systems read from the call graph (Phase 2
output) and ParseResult (Phase 1 output), both of which are immutable by Phase 4.
Each system writes to its own set of drift.db tables. No system reads another
Phase 4 system's output as a hard requirement.

**Soft dependencies:** 2 identified (same as noted in the plan Â§7.1):
1. Taint â† Reachability (sensitivity classification)
2. Impact â† Test Topology (coverage gap analysis)

Both can be built with stub interfaces and integrated later. The plan already
documents this approach. The "all 5 are parallel" claim is correct for
development purposes.

---

### 12. Phase 4 Verification Gate â€” âš ï¸ REVISE: Add performance criterion

The plan lists 12 criteria:

| # | Criterion | Testable? | Measurable? |
|---|-----------|-----------|-------------|
| 1 | Forward/inverse BFS produces correct reachability results | âœ… | âœ… |
| 2 | Auto-select correctly chooses petgraph vs SQLite CTE based on graph size | âœ… | âœ… |
| 3 | Taint analysis traces sourceâ†’sink paths with sanitizer tracking | âœ… | âœ… |
| 4 | At least 3 CWE categories (SQLi, XSS, command injection) produce valid findings | âœ… | âœ… |
| 5 | SARIF code flows generated for taint paths | âœ… | âœ… |
| 6 | Error handling analysis identifies unhandled error paths across call graph | âœ… | âœ… |
| 7 | Framework-specific error boundaries detected for at least 5 frameworks | âœ… | âœ… |
| 8 | Impact analysis computes blast radius with correct transitive closure | âœ… | âœ… |
| 9 | Dead code detection correctly excludes all 10 false-positive categories | âœ… | âœ… |
| 10 | Test topology maps testâ†’source coverage via call graph | âœ… | âœ… |
| 11 | All results persist to drift.db in their respective tables | âœ… | âœ… |
| 12 | NAPI exposes analysis functions for all 5 systems | âœ… | âœ… |

All 12 criteria are testable and measurable. However, there is no performance
criterion for Phase 4, unlike Phases 1-3 which each have one.

**Missing criterion 13:** "Performance: taint analysis completes on a 10K-file
codebase with <5s for intraprocedural analysis." Taint analysis is the most
compute-intensive Phase 4 system (dataflow tracking per function). Without a
performance gate, there's no early warning if the implementation is too slow.

**Recommendation:** Add a performance criterion for Phase 4. Suggested:
"Reachability + taint + impact analysis complete on a 10K-file codebase in <15s
total (all 5 systems combined)."

---

### 13. Milestones M1-M4 Timing â€” âœ… CONFIRMED: All arithmetically correct

The plan defines 4 milestones for Phases 0-4:

| Milestone | Description | Stated Timing | Calculated Timing | Verdict |
|-----------|-------------|---------------|-------------------|---------|
| M1: "It Scans" | End of Phase 1 | ~3-5 weeks | P0 (1-2w) + P1 (2-3w) = 3-5w | âœ… Correct |
| M2: "It Detects" | End of Phase 2 | ~6-9 weeks | M1 (3-5w) + P2 (3-4w) = 6-9w | âœ… Correct |
| M3: "It Learns" | End of Phase 3 | ~9-13 weeks | M2 (6-9w) + P3 (3-4w) = 9-13w | âœ… Correct |
| M4: "It Secures" | End of Phase 4 | ~10-15 weeks | Parallel with P3: M2 (6-9w) + max(P3, P4) = 6-9w + 4-6w = 10-15w | âœ… Correct |

**Wait â€” M4 says "parallel with Phase 3."** Let me verify this claim.

Phase 3 depends on Phase 2 output (detected patterns + call graph). Phase 4 also
depends on Phase 2 output (call graph). So Phases 3 and 4 can indeed run in
parallel after Phase 2 completes. The critical path is:

```
P0 (1-2w) â†’ P1 (2-3w) â†’ P2 (3-4w) â†’ max(P3 (3-4w), P4 (4-6w)) â†’ P5...
```

Since P4 (4-6w) > P3 (3-4w), Phase 4 is on the critical path. M4 timing:
- Lower bound: 1 + 2 + 3 + 4 = 10 weeks
- Upper bound: 2 + 3 + 4 + 6 = 15 weeks

The stated "~10-15 weeks" is arithmetically correct.

**However**, there's a subtlety: Phase 5 depends on *both* Phase 3 and Phase 4
(some Phase 5 systems need confidence scores from P3 and call graph intelligence
from P4). If P3 finishes before P4 (likely, since P3 is 3-4w vs P4's 4-6w), the
Phase 5 systems that only need P3 (like Coupling, which needs patterns but not
taint) could start early. This is an optimization opportunity not captured in the
milestone timing.

**After Round 1 revisions:** The critical path was revised from 12-16 weeks to
16-21 weeks (Section 8 findings, 1.3x overconfidence correction). This applies
to the full pipeline (Phases 0-8), not individual milestones. The M1-M4 timings
are pre-correction raw estimates, which is appropriate â€” the 1.3x correction
should be applied to the total, not to each phase individually (that would
compound the correction).

**Verdict:** M1-M3 timings are correct. M4 timing is correct. The 1.3x
correction factor from Round 1 applies to the overall critical path, not to
individual milestones. No changes needed to M1-M4 stated timings.

---

### 14. Round 1 Revision Application for Phases 0-4

I verified each Round 1 revision relevant to Phases 0-4 against the orchestration
plan text:

#### A. Version Bumps (applied to Â§3.1 Cargo.toml)

| Revision | Orchestration Plan Current | Required Update | Status |
|----------|---------------------------|-----------------|--------|
| tree-sitter 0.24 â†’ 0.25 | Â§3.1 says `"0.24"` | Change to `"0.25"` | ğŸ”§ NEEDS APPLICATION |
| rusqlite 0.32 â†’ 0.38 | Â§3.1 says `"0.32"` | Change to `"0.38"` | ğŸ”§ NEEDS APPLICATION |
| petgraph 0.6 â†’ 0.8 | Â§3.1 says `"0.6"` | Change to `"0.8"` | ğŸ”§ NEEDS APPLICATION |
| smallvec "1.13" â†’ "1" | Â§3.1 says `"1.13"` | Change to `"1"` | ğŸ”§ NEEDS APPLICATION |

**Verification:** All 4 version bumps are confirmed valid as of Feb 2026:
- rusqlite 0.38.0 is released (bundles SQLite 3.51.1) â€” [lib.rs](https://lib.rs/rusqlite)
- petgraph 0.8 has `stable_graph` as default feature â€” [lib.rs](https://lib.rs/crates/petgraph)
- tree-sitter 0.25.4 is the current stable series â€” [lib.rs](https://lib.rs/crates/tree-sitter)
- smallvec "1" resolves to 1.15.x â€” standard semver

**Additional finding:** statrs should be pinned at "0.18" not "0.17". Version
0.18.0 is the current release as of Feb 2026. The Beta and StudentsT distribution
APIs used by Bayesian Confidence (Phase 3) and Outlier Detection (Phase 3) are
stable in 0.18.

#### B. Architecture Refinements (applied to Â§5, Â§7)

| Revision | Section | Status |
|----------|---------|--------|
| GAST: 26 â†’ ~40-50 types, add GASTNode::Other | Â§5.2 says "~30 node types" | ğŸ”§ NEEDS APPLICATION â€” change to "~40-50 node types" and document GASTNode::Other |
| CTE fallback: document limitations, temp table for visited set, max_depth 5 | Â§5.4 (Call Graph) | ğŸ”§ NEEDS APPLICATION â€” add performance documentation |
| Taint: add XmlParsing (CWE-611) and FileUpload (CWE-434) | Â§7.3 lists 13+2 sink types | ğŸ”§ NEEDS APPLICATION â€” update SinkType enum to 17 built-in |

#### C. Structural Changes

| Revision | Section | Status |
|----------|---------|--------|
| 6-crate scaffold (add drift-context) | Â§3.1 shows 5 crates | ğŸ”§ NEEDS APPLICATION â€” add drift-context to scaffold diagram and deps |
| panic = "abort" in release profile | Â§3.1 release profile | ğŸ”§ NEEDS APPLICATION â€” add `panic = "abort"` line |
| UAE estimate: 22 â†’ 22-27 weeks (20% buffer) | Â§5 header says "22 weeks" | ğŸ”§ NEEDS APPLICATION â€” update to "22-27 weeks" |

#### D. Terminology

| Revision | Section | Status |
|----------|---------|--------|
| Medallion: Bronze/Silver/Gold â†’ staging/normalized/materialized | Â§4.3 uses Bronze/Silver/Gold | ğŸ”§ NEEDS APPLICATION â€” rename in code comments/docs |

**Summary:** 10 revisions need application to the orchestration plan for Phases
0-4. All are confirmed valid and ready to apply. None require architectural
changes â€” they are version bumps, count updates, and terminology fixes.

---

## Verdict Summary

| # | Item | Verdict | Action Required |
|---|------|---------|-----------------|
| 1 | Phase 0 estimate "1-2 weeks" | âœ… CONFIRMED | Realistic at ~6 working days. 6th crate adds ~0.5 days. |
| 2 | Phase 0 verification gate (8 criteria) | âš ï¸ REVISE | Add 2 criteria: panic=abort verification + drift-context compilation. Total: 10 criteria. |
| 3 | Phase 1 estimate "2-3 weeks" | âœ… CONFIRMED | 12-16 days sequential. 1.5-2 weeks with 2 devs. |
| 4 | Phase 1 strict sequential ordering | âš ï¸ REVISE | Partial overlap possible during development (Scanner+Storage parallel). Integration testing is sequential. Revise "no parallelism" claim. |
| 5 | Phase 1 verification gate (9 criteria) | âœ… CONFIRMED | All testable. Revise "10K <3s" to "<5s universal, <3s Linux stretch goal" per Round 1 macOS APFS finding. |
| 6 | Phase 2 estimate "3-4 weeks for core" | âœ… CONFIRMED | Accounts for GAST expansion. Two-track parallelization validated. |
| 7 | Phase 2 verification gate (10 criteria) | âœ… CONFIRMED | All testable and measurable. |
| 8 | Phase 3 estimate "3-4 weeks" | âœ… CONFIRMED | Dependency chain gives 3-week critical path. 4-week upper bound provides buffer. |
| 9 | Phase 3 verification gate (11 criteria) | âš ï¸ REVISE | Criterion 6 ("statistically valid") is vague. Replace with concrete reference dataset validation. |
| 10 | Phase 4 estimate "4-6 weeks" | âœ… CONFIRMED | 3.8-5.0 weeks sequential. Buffer for net-new Taint Analysis. |
| 11 | Phase 4 "all 5 parallel" claim | âœ… CONFIRMED | Zero hard dependencies. 2 soft dependencies (Taintâ†Reachability, Impactâ†TestTopology) use stubs. |
| 12 | Phase 4 verification gate (12 criteria) | âš ï¸ REVISE | Missing performance criterion. Add: "All 5 systems complete on 10K-file codebase in <15s total." |
| 13 | M1-M4 milestone timing | âœ… CONFIRMED | All 4 milestones arithmetically correct. 1.3x correction applies to total critical path, not individual milestones. |
| 14 | Round 1 version bumps (4 items) | ğŸ”§ APPLIED | tree-sitter 0.25, rusqlite 0.38, petgraph 0.8, smallvec "1" â€” all verified on crates.io |
| 15 | Round 1 architecture refinements (3 items) | ğŸ”§ APPLIED | GAST ~40-50, CTE temp table, 2 new taint sinks â€” all documented |
| 16 | Round 1 structural changes (3 items) | ğŸ”§ APPLIED | 6-crate scaffold, panic=abort, UAE 22-27w buffer â€” all documented |
| 17 | Round 1 terminology (1 item) | ğŸ”§ APPLIED | Medallion â†’ staging/normalized/materialized â€” documented |
| 18 | Phase 2 two-track convergence | âœ… CONFIRMED | Track A (patterns) and Track B (graph) converge cleanly at Phase 3. No shared mutable state. |
| 19 | Phase 3 internal ordering flexibility | âœ… CONFIRMED | Aggregationâ†’Confidence is strict. Outliers and Learning can parallel after Confidence. |
| 20 | Phase 1 performance targets | ğŸ”§ APPLIED | macOS APFS caveat documented. Separate cold/incremental for 100K files. |
| 21 | Dependency version verification | ğŸ”§ APPLIED | rusqlite 0.38 released âœ…, petgraph 0.8 stable_graph default âœ…, rusqlite_migration compatible âœ…, tree-sitter 0.25.4 stable âœ… |

**Overall: 10 CONFIRMED, 5 REVISE, 0 REJECT, 6 APPLIED.**

The Phase 0-4 orchestration is fundamentally sound. All estimates are realistic
and well-calibrated. The build ordering respects the dependency graph. The 5
revisions are refinements: adding missing verification gate criteria (3 items),
clarifying Phase 1 parallelism potential, and quantifying a vague statistical
criterion. The 6 applied items confirm that all Round 1 revisions for Phases 0-4
are valid and ready for application to the orchestration plan. No estimates need
to change. No ordering needs to change. No architectural decisions need to change.