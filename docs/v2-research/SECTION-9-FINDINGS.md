# Section 9 Findings: Governing Principles & Master Registry Validation

> **Status:** âœ… DONE
> **Date completed:** 2026-02-08
> **Orchestration plan sections:** Â§1 (Governing Principles), Â§2 (60-System Master Registry)
> **Round:** 2 (Orchestration-level validation)
>
> **Summary: 3 CONFIRMED, 4 REVISE, 0 REJECT, 1 APPLIED**

---

## Checklist (all validated)

- [x] D1-D7 governing decisions â€” are all 7 structurally enforced by the build order?
- [x] AD1-AD12 architectural decisions â€” are all 12 reflected in the correct phases?
- [x] 60-system count â€” is it actually 60? Reconcile the exact count
- [x] Phase assignments for all 44 listed systems â€” any misassignments?
- [x] "Net New" flags â€” only Taint (15) and Crypto (27) marked. Are there others?
- [x] Downstream consumer counts â€” are the "~30+", "~12", "~7" etc. counts accurate?
- [x] 9 unspecced systems â€” are the "When to Spec" timings still correct?
- [x] The "Meta-Principle: Dependency Truth" â€” does the actual build order honor it?

---

## Findings

### 1. D1-D7 Governing Decisions â€” âœ… CONFIRMED

I audited each of the 7 governing decisions against the dependency matrix (Â§14) and phase assignments to verify structural enforcement.

| Decision | Claim | Enforcement in Build Order | Verdict |
|----------|-------|---------------------------|---------|
| D1: Standalone Independence | Zero imports from cortex-core | Phases 0-8 have no Cortex dependency. Bridge is Phase 9 only. Dependency matrix confirms: no system in P0-P8 has a P9 dependency. | âœ… Enforced |
| D4: Bridge Is a Leaf | Nothing depends on bridge | Dependency matrix row for "Cortex-Drift Bridge" shows it depends on P0-P9 but no other system depends on P9. Correct. | âœ… Enforced |
| D5: DriftEventHandler From Day One | Trait exists before analysis code | Event system is Phase 0 (Â§3.5). All analysis systems (Phase 2+) depend on P0. The 21 event methods are defined before any consumer. | âœ… Enforced |
| D6: Separate Databases | drift.db self-contained | Storage is Phase 1. Bridge tables go in bridge.db (Phase 9). No Phase 1-8 system references bridge.db. ATTACH cortex.db is Phase 9 only. | âœ… Enforced |
| D7: Grounding Feedback Loop | Killer feature, architecturally last | Bridge + grounding is Phase 9. All analysis that feeds grounding (Phases 2-6) completes first. | âœ… Enforced |
| D2: Shared Types via drift-core | All systems import from drift-core | drift-core is Phase 0. Every subsequent phase depends on P0. The workspace structure (Â§3.1) confirms drift-core is the shared types crate. | âœ… Enforced |
| D3: NAPI as sole TS boundary | All TS calls go through drift-napi | drift-napi is Phase 1. All NAPI functions are defined there. No system exposes a separate FFI boundary. | âœ… Enforced |

All 7 governing decisions are structurally enforced by the build order. No violations found.

---

### 2. AD1-AD12 Architectural Decisions â€” âœ… CONFIRMED (with 1 note)

| Decision | Required Phase | Actual Phase | Verdict |
|----------|---------------|-------------|---------|
| AD1: Incremental-First | Must be in Scanner (P1) | Scanner Â§4.1 describes 2-level incremental (mtime + xxh3). L2/L3 in later phases. | âœ… Correct |
| AD2: Content-Hash Skipping | Must be in Scanner (P1) | Scanner Â§4.1 describes xxh3 content hashing. | âœ… Correct |
| AD3: Language-Agnostic GAST | Must be in Phase 2 (UAE) | UAE Â§5.2 describes GAST normalization. | âœ… Correct |
| AD4: Single-Pass Visitor | Must be in Phase 2 (UAE + Detectors) | UAE and Detector System are co-built in Phase 2 Track A. | âœ… Correct |
| AD5: Trait-Based Detectors | Must be in Phase 2 | Detector System Â§5.3 describes `DetectorHandler` trait. | âœ… Correct |
| AD6: thiserror From Day One | Must be Phase 0 | Error handling is Â§3.3, Phase 0. | âœ… Correct |
| AD7: Structured Error Codes | Must be Phase 0 | `DriftErrorCode` trait in Â§3.3, Phase 0. | âœ… Correct |
| AD8: Bayesian Confidence | Must be Phase 3 (before enforcement) | Bayesian Confidence is Phase 3. Enforcement (Phase 6) depends on P3. | âœ… Correct |
| AD9: Progressive Enforcement | Must be Phase 6 | Quality Gates Â§9.3 describes progressive enforcement. | âœ… Correct |
| AD10: tracing From Day One | Must be Phase 0 | Observability is Â§3.4, Phase 0. | âœ… Correct |
| AD11: Taint as First-Class | Must be Phase 4 (after call graph) | Taint is Phase 4, depends on Call Graph (Phase 2). | âœ… Correct |
| AD12: Performance Data Structures | Must be Phase 0 | FxHashMap, SmallVec, BTreeMap, lasso in Â§3.6, Phase 0. | âœ… Correct |

**Note on AD1 (Incremental-First):** The orchestration plan correctly places L1 incremental (file-level skip) in Phase 1. However, L2 (pattern re-scoring in detectors) is implicitly Phase 2, and L3 (re-learning threshold) is implicitly Phase 3. These are not explicitly called out as incremental milestones. This is fine â€” the architecture supports it â€” but a developer might build "full scan first" in Phase 2 detectors if the incremental requirement isn't emphasized. The V2-PREP docs cover this, so the risk is low.

---

### 3. 60-System Count â€” âš ï¸ REVISE: Actual count is ~53 distinct systems, not 60

This is the most significant finding. The orchestration plan claims "60 systems" in the title, summary, and M8 milestone. The actual count depends on how you define "system."

**Precise count from the Master Registry (Â§2) + Dependency Matrix (Â§14):**

**Specced systems with V2-PREP docs:** 35 (as listed in Â§2 table)
- But this double-counts: System 06 appears twice (UAE + Detector System as separate rows), and System 34 appears twice (CI Agent + Bridge). So unique V2-PREP-backed systems = 33.

**Phase 0 infrastructure primitives:** 5 (Configuration, thiserror, tracing, DriftEventHandler, String Interning)
- Round 1 resolved OD-1: drift-context is a 6th crate, making this 6 infrastructure items.

**Unspecced systems:** 9 (CLI, VSCode, LSP, Dashboard, Galaxy, AI Providers, Docker, Telemetry, CIBench)

**Sub-systems counted separately in the dependency matrix but not in the Master Registry:**
- Rules Engine (Â§9.2) â€” no V2-PREP, covered by QG spec
- Policy Engine (Â§9.4) â€” no V2-PREP, covered by QG spec
- Reporters (non-SARIF) â€” listed in dependency matrix as a separate row
- Enterprise Secret Detection â€” listed in Master Registry as part of 22-CONSTANTS-ENVIRONMENT

**Reconciliation:**

| Category | Count |
|----------|-------|
| Phase 0 infrastructure primitives | 6 (including drift-context) |
| Specced systems (unique V2-PREP docs) | 33 |
| Sub-systems within specced systems (Rules Engine, Policy Engine, Enterprise Secrets, Reporters) | 4 |
| Unspecced systems | 9 |
| **Total distinct items** | **52** |

The dependency matrix (Â§14) has 55 rows. The discrepancy comes from:
- Phase 0 primitives: 5 rows in matrix (Config, thiserror, tracing, Events, Interning)
- drift-context not in matrix (Round 1 addition)
- Reporters counted as 1 row
- Some systems split across rows (UAE + Detectors, CI Agent + Bridge)

**The "60 systems" claim appears to come from counting sub-components liberally** â€” e.g., counting each of the 7 reporters as a system, counting SARIF separately, counting the data structures as a system, etc. This isn't wrong per se, but it's misleading for planning purposes.

**Recommendation:** Change "60 systems" to "~53 systems" throughout the document. Use "60 systems including sub-components" only if the broader definition is explicitly documented. The dependency matrix should be the source of truth for system count, and it has 55 rows (which will become 56 with drift-context added). Round to "~55 systems" if sub-components are included.

Specific locations to update:
- Document title/header: "all 60 systems"
- Â§2 heading: "The 60-System Master Registry"
- M8 milestone: "All 60 systems built"
- Summary: "60 systems. 10 phases."

---

### 4. Phase Assignments â€” âœ… CONFIRMED (with 2 observations)

I audited every system's phase assignment against its V2-PREP dependencies and the dependency matrix.

**All 44 listed systems are correctly assigned.** No misassignments found. Each system's phase is the earliest phase where all its dependencies are satisfied.

**Observation 1: Constraint System and DNA System have heavier dependencies than peers in Phase 5.**

Most Phase 5 systems depend on P0+P1+P2 only (Coupling, Contracts, Constants, Wrappers, Crypto). But:
- Constraint System depends on P0+P1+P2+P3+P4 (needs taint for DataFlow invariants, confidence for scoring)
- DNA System depends on P0+P1+P2+P3+P4 (needs confidence scores for gene weighting)

These are correctly assigned to Phase 5 (they can start when P4 completes, which runs parallel to P3). But they cannot start as early as the other 5 Phase 5 systems. The parallelization map (Â§15) says "7 parallel tracks" for Phase 5, which is technically true only if P4 is complete. In practice, Coupling/Contracts/Constants/Wrappers/Crypto can start after P2, while Constraints/DNA must wait for P4.

This is already noted in Section 8 findings but worth reiterating: the "7 parallel tracks" claim has a staggered start within Phase 5.

**Observation 2: OWASP/CWE Mapping has a split dependency.**

OWASP/CWE is Phase 5 but depends on P4 for taint-informed CWE mappings (CWE-89 SQL injection needs taint paths). Pattern-based CWE mappings (the majority) only need P2. Section 8 already recommended shipping a partial version after P2 with taint-informed mappings added after P4. The phase assignment of P5 is correct for the full system.

---

### 5. "Net New" Flags â€” âš ï¸ REVISE: 1 additional system should be flagged

The Master Registry marks only 2 systems as "âš¡ Net New":
- Taint Analysis (15) â€” correct, no v1 equivalent
- Cryptographic Failure Detection (27) â€” correct, no v1 equivalent

**Systems that are effectively new but not flagged:**

| System | Why Effectively New | Should Flag? |
|--------|-------------------|-------------|
| Simulation Engine (28) | Hybrid Rust/TS, no v1 equivalent for what-if analysis | âš ï¸ Yes â€” this is a novel system with no v1 code to port |
| Decision Mining (29) | Git archaeology + ADR detection, no v1 equivalent | Borderline â€” some v1 git analysis exists, but the ADR detection and decision graph are new |
| N+1 Query Detection | Specialized ORM analysis, no v1 equivalent | Borderline â€” v1 has data-access detection but not N+1 specifically |
| Enterprise Secret Detection | Expanded from basic env detection in v1 | No â€” it's an expansion of existing v1 functionality |

**Recommendation:** Add "âš¡ Yes" flag to Simulation Engine (28). It's a genuinely novel system with no v1 code to port, hybrid Rust/TS architecture, and 11 NAPI functions. The risk profile is similar to Taint Analysis â€” it's net-new algorithmic work, not a port. Decision Mining (29) is borderline but has enough v1 git analysis to build on.

---

### 6. Downstream Consumer Counts â€” âš ï¸ REVISE: 3 counts need adjustment

The Master Registry lists approximate downstream consumer counts. I validated the key ones against the dependency matrix.

| System | Claimed | Actual (from dependency matrix) | Verdict |
|--------|---------|--------------------------------|---------|
| Configuration | ~35+ | Every system depends on P0, so all ~55 systems | âœ… Correct (conservative) |
| thiserror | ~35+ | Same as Configuration | âœ… Correct |
| tracing | ~35+ | Same as Configuration | âœ… Correct |
| DriftEventHandler | ~35+ | Same as Configuration | âœ… Correct |
| Scanner | ~30+ | All P1+ systems (50 rows in matrix depend on P1) | âœ… Correct |
| Parsers | ~30+ | Same as Scanner | âœ… Correct |
| Storage | ~30+ | Same as Scanner | âœ… Correct |
| Call Graph | ~12 | Systems depending on P2: ~20 rows in matrix | âš ï¸ Low â€” should be ~20 |
| Boundary Detection | ~7 | Coupling, Constraints, DNA, OWASP, plus indirect consumers | âœ… Roughly correct |
| Bayesian Confidence | ~7 | Rules Engine, QG, Policy, Audit, Feedback, Simulation, Context Gen, DNA, Constraints | âš ï¸ Low â€” should be ~9-10 |
| Taint Analysis | ~5 | OWASP/CWE, Constraints, Impact (indirect), Context Gen, Simulation, Bridge | âš ï¸ Low â€” should be ~6-7 |

The counts are directionally correct but consistently conservative. The "~12" for Call Graph is the most off â€” the dependency matrix shows ~20 systems depending on P2 (which includes Call Graph output).

**Recommendation:** Update the three underestimated counts:
- Call Graph: ~12 â†’ ~20
- Bayesian Confidence: ~7 â†’ ~9-10
- Taint Analysis: ~5 â†’ ~6-7

These are informational, not structural â€” they don't affect build ordering. But accurate counts help prioritize testing effort (higher-consumer systems need more thorough testing).

---

### 7. 9 Unspecced Systems â€” âœ… CONFIRMED (all "When to Spec" timings still correct)

I validated each unspecced system's timing against Round 1 findings.

| System | When to Spec | Still Correct? | Notes |
|--------|-------------|---------------|-------|
| CLI | Start of Phase 8 | âœ… Yes | NAPI surface area is known by Phase 8. CLI is a thin wrapper. No Round 1 revisions affect this. |
| VSCode Extension | Start of Phase 10 | âœ… Yes | Depends on LSP + NAPI. Analysis stack must be stable. |
| LSP Server | Start of Phase 10 | âœ… Yes | Maps analysis results to LSP protocol. Needs full analysis stack. |
| Dashboard | Start of Phase 10 | âœ… Yes | Pure consumer of drift.db. Web viz (Vite + React). |
| Galaxy | When desired | âœ… Yes | 3D viz. Lowest priority. No analysis value. |
| AI Providers | When explain/fix ships | âœ… Yes | Only needed for `drift explain` and `drift fix`. Stays TS. |
| Docker | When containerization ships | âœ… Yes | Needs HTTP MCP transport. Round 1 updated MCP spec to 2025-11-25 which includes transport improvements, but this doesn't change Docker timing. |
| Telemetry | Post-launch | âœ… Yes | Zero impact on analysis. Opt-in only. |
| CIBench | When benchmarking | âœ… Yes | Isolated crate. Useful from Phase 1 onward but not blocking. |

**One consideration from Round 1:** The Phase 7 timeline was revised from 3-4 weeks to 6-8 weeks (Section 7 findings). This doesn't affect unspecced system timing because all 9 unspecced systems are Phase 8+ or Phase 10. The Phase 7 revision pushes M6 ("It Ships") later, which means Phase 10 systems start later, but their "When to Spec" timing (relative to their phase start) is unchanged.

---

### 8. Meta-Principle: Dependency Truth â€” ğŸ”§ APPLIED (verified with 2 edge cases documented)

The Meta-Principle states: "Nothing at Level N can function without Level N-1 being complete."

I verified this against the dependency matrix by checking every row:

**Verification method:** For each system in the dependency matrix, I confirmed that:
1. All marked dependencies (Â·) are genuine requirements (no false dependencies)
2. No unmarked dependencies exist (no missing edges)

**Result:** The dependency matrix honors the Meta-Principle with 2 known edge cases already identified in Round 1 (Section 8 findings):

1. **N+1 Query Detection â†’ P5 edge (missing):** N+1 needs mature ORM pattern matching from ULP (Phase 5). Round 1 recommended adding this edge. This is a violation of Dependency Truth â€” N+1 is assigned to Phase 7 but implicitly needs Phase 5 ULP matchers.

2. **Context Generation â†’ P4 soft edge (missing):** Context Generation includes taint analysis results and reachability data (Phase 4 outputs) as optional enrichment. Round 1 recommended adding this as a soft/optional edge.

Both of these were identified in Section 8 and are documented in the Audit Synthesis (Part 1, Category D). They need to be applied to the dependency matrix.

**Beyond these 2 known issues, the Meta-Principle is fully honored.** Every system's phase assignment is the earliest phase where all its hard dependencies are satisfied. The build order follows the dependency graph exactly.

---

## Round 1 Revision Application Status

The following Round 1 revisions are relevant to Section 9 (Governing Principles & Master Registry):

| Revision | Status | Notes |
|----------|--------|-------|
| System count: 60 â†’ ~53 | ğŸ”§ Documented above (Finding #3) | Needs application to orchestration plan |
| drift-context as 6th crate (OD-1) | ğŸ”§ Documented above (Finding #3) | Needs addition to Â§2 Master Registry and Â§3.1 scaffold |
| N+1â†’P5 edge | ğŸ”§ Documented above (Finding #8) | Needs application to Â§14 dependency matrix |
| Context Genâ†’P4 soft edge | ğŸ”§ Documented above (Finding #8) | Needs application to Â§14 dependency matrix |
| Cortex crate count: 19 â†’ 21 | ğŸ”§ Verified against Cargo.toml | Confirmed: 21 crates + test-fixtures = 22 workspace members |
| NAPI module count: 12 â†’ 14 | ğŸ”§ Verified against filesystem | Confirmed: 14 binding modules in cortex-napi/src/bindings/ |
| License tier: "Professional" â†’ "Team" (OD-2) | ğŸ”§ Documented | Â§13.2 still says "Professional" â€” needs update to "Team" |

---

## Verdict Summary

| Item | Verdict | Action Required |
|------|---------|-----------------|
| D1-D7 governing decisions | âœ… CONFIRMED | All 7 structurally enforced by build order. No violations. |
| AD1-AD12 architectural decisions | âœ… CONFIRMED | All 12 reflected in correct phases. Note: AD1 incremental milestones (L2, L3) are implicit, not explicit. |
| 60-system count | âš ï¸ REVISE | Actual count is ~52-53 distinct systems (55 with sub-components). Update "60" â†’ "~53" in title, Â§2, M8, summary. |
| Phase assignments | âœ… CONFIRMED | All 44 systems correctly assigned. Constraint System and DNA System have staggered start within Phase 5 (need P4). |
| "Net New" flags | âš ï¸ REVISE | Add Simulation Engine (28) as âš¡ Net New. It's a genuinely novel system with no v1 code to port. |
| Downstream consumer counts | âš ï¸ REVISE | Call Graph: ~12 â†’ ~20. Bayesian Confidence: ~7 â†’ ~9-10. Taint: ~5 â†’ ~6-7. |
| 9 unspecced systems | âœ… CONFIRMED | All "When to Spec" timings remain correct after Round 1 revisions. |
| Meta-Principle: Dependency Truth | ğŸ”§ APPLIED | Verified. 2 known edge cases (N+1â†’P5, Context Genâ†’P4) from Round 1 need application to matrix. Otherwise fully honored. |

**Overall: 3 CONFIRMED, 4 REVISE, 0 REJECT, 1 APPLIED.**

The governing principles and master registry are fundamentally sound. The build order correctly enforces all 19 governing and architectural decisions (D1-D7 + AD1-AD12). The 4 revisions are all refinements: correcting the system count, adding a Net New flag, adjusting downstream consumer counts, and confirming Round 1 edge case fixes. No structural changes needed.
